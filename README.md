# Teddy Crawler ğŸ»

Playwrightãƒ™ãƒ¼ã‚¹ã®æ±ç”¨Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼åŸºç›¤

## æ¦‚è¦

Teddy Crawlerã¯ã€JavaScriptå‹•çš„ã‚µã‚¤ãƒˆã«å¯¾å¿œã—ãŸWebã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã§ã™ã€‚URLã‚’æŒ‡å®šã™ã‚‹ã ã‘ã§ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒãƒ»ãƒªãƒ³ã‚¯ã‚’è‡ªå‹•åé›†ã—ã€JSONå½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚

## ç‰¹å¾´

- ğŸ­ **Playwright powered**: JavaScriptå‹•çš„ã‚µã‚¤ãƒˆå®Œå…¨å¯¾å¿œ
- âš™ï¸  **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é§†å‹•**: YAMLè¨­å®šã§æŸ”è»Ÿãªã‚¯ãƒ­ãƒ¼ãƒ«å®šç¾©
- ğŸ–¥ï¸  **CLIå¯¾å¿œ**: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰æ‰‹è»½ã«æ“ä½œ
- ğŸ“‹ **æ±ç”¨æŠ½å‡º**: ãƒ†ã‚­ã‚¹ãƒˆãƒ»ãƒªãƒ³ã‚¯ãƒ»ç”»åƒã‚’è‡ªå‹•æŠ½å‡º
- ğŸ¯ **ã‚»ãƒ¬ã‚¯ã‚¿å¯¾å¿œ**: CSS/XPathã‚»ãƒ¬ã‚¯ã‚¿ã§ç‰¹å®šè¦ç´ ã‚’å–å¾—
- â³ **ãƒ¬ãƒ¼ãƒˆåˆ¶é™**: ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è€ƒæ…®ã—ãŸé–“éš”åˆ¶å¾¡
- ğŸ“¸ **ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ**: ãƒšãƒ¼ã‚¸ã®è¦‹ãŸç›®ã‚‚ä¿å­˜å¯èƒ½
- ğŸª **Cookie/Profileå¯¾å¿œ**: ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¶­æŒ
- ğŸ“Š **çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ**: ã‚¯ãƒ­ãƒ¼ãƒ«çµæœã‚’ã‚µãƒãƒªãƒ¼å‡ºåŠ›

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### å‰ææ¡ä»¶

- Python 3.9+
- Playwrightï¼ˆæ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ï¼‰
- PyYAMLï¼ˆæ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ï¼‰

### Playwrightãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Chromiumãƒ–ãƒ©ã‚¦ã‚¶ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
python3 -m playwright install chromium
```

## åŸºæœ¬çš„ãªä½¿ã„æ–¹

### 1. å˜ä¸€URLå–å¾—

```bash
# åŸºæœ¬çš„ãªå–å¾—
python3 crawler.py fetch https://example.com

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãã§å–å¾—
python3 crawler.py fetch https://example.com --screenshot --wait 5000 --scroll
```

### 2. ãƒãƒƒãƒå‡¦ç†

```bash
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒƒãƒã‚¯ãƒ­ãƒ¼ãƒ«
python3 crawler.py batch config.yaml

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
python3 crawler.py batch my_custom_config.yaml --output my_results.json
```

### 3. å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª

```bash
# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º
python3 crawler.py list
```

## è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆconfig.yamlï¼‰

### åŸºæœ¬æ§‹æˆ

```yaml
# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
settings:
  delay: 2000                           # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆãƒŸãƒªç§’ï¼‰
  timeout: 30000                        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆãƒŸãƒªç§’ï¼‰
  user_agent: "TeddyCrawler/1.0"        # User-Agent
  screenshot: false                     # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆå–å¾—
  profile: null                         # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å
  output_format: "json"                 # å‡ºåŠ›å½¢å¼

# ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¸ãƒ§ãƒ–å®šç¾©
jobs:
  - name: "example_job"
    url: "https://example.com"
    extract: ["text", "links", "images"]
    wait: 2000
    scroll: false
    screenshot: false
```

### ã‚µãƒ³ãƒ—ãƒ«è¨­å®š

#### note.com ãƒˆãƒ¬ãƒ³ãƒ‰è¨˜äº‹

```yaml
jobs:
  - name: "note_trending"
    url: "https://note.com/search?q=AI&sort=trend"
    selectors:
      title: "h3 a"
      link: "a[href*='/n/']"
      author: ".note-common-authors a"
    wait: 3000
    scroll: true
    scroll_count: 3
    extract: ["text", "links"]
```

#### GitHub ãƒªãƒã‚¸ãƒˆãƒªæƒ…å ±

```yaml
jobs:
  - name: "github_repo"
    url: "https://github.com/microsoft/playwright"
    selectors:
      repo_name: "h1[data-view-component=true] strong a"
      description: "p[data-view-component=true]"
      stars: "#repo-stars-counter-star"
      language: "[data-view-component=true][data-testid*=language]"
    extract: ["text", "links"]
    wait: 2000
```

## ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³

### fetchã‚³ãƒãƒ³ãƒ‰

```bash
python3 crawler.py fetch <URL> [ã‚ªãƒ—ã‚·ãƒ§ãƒ³]

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  -o, --output FILENAME     å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
  -s, --screenshot          ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—
  -p, --profile PROFILE     ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒ‡å®š
  -ua, --user-agent AGENT   User-Agentæ–‡å­—åˆ—
  -w, --wait MILLISECONDS   å¾…æ©Ÿæ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰
  --scroll                  ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œ
  --headful                 ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
```

### batchã‚³ãƒãƒ³ãƒ‰

```bash
python3 crawler.py batch <CONFIG_FILE> [ã‚ªãƒ—ã‚·ãƒ§ãƒ³]

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  -o, --output FILENAME     å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
```

## æŠ½å‡ºå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿

### ãƒ†ã‚­ã‚¹ãƒˆï¼ˆtextï¼‰

- ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
- ãƒ¡ã‚¤ãƒ³è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
- æ®µè½ãƒ†ã‚­ã‚¹ãƒˆ
- è¦‹å‡ºã—ï¼ˆh1-h6ï¼‰
- ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³

### ãƒªãƒ³ã‚¯ï¼ˆlinksï¼‰

- å†…éƒ¨ãƒªãƒ³ã‚¯ãƒ»å¤–éƒ¨ãƒªãƒ³ã‚¯
- ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ãƒ»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªãƒ³ã‚¯
- ç”»åƒä»˜ããƒªãƒ³ã‚¯
- ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚¿ã‚¤ãƒˆãƒ«

### ç”»åƒï¼ˆimagesï¼‰

- imgè¦ç´ ã®ç”»åƒ
- CSSèƒŒæ™¯ç”»åƒ
- altå±æ€§ãƒ»titleå±æ€§
- ç”»åƒã‚µã‚¤ã‚ºãƒ»å½¢å¼åˆ¥çµ±è¨ˆ

## ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ©Ÿèƒ½

### æ—¢å­˜ã®teddy_browserãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ©ç”¨

```yaml
settings:
  profile: "default"  # /home/ec2-user/tools/teddy_browser/profiles/default ã‚’ä½¿ç”¨
```

### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ

```bash
# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
mkdir -p profiles/my_profile
```

```yaml
settings:
  profile: "my_profile"  # ./profiles/my_profile ã‚’ä½¿ç”¨
```

## å‡ºåŠ›å½¢å¼

### JSONå½¢å¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

```json
{
  "metadata": {
    "crawled_at": "2024-01-15T10:30:00Z",
    "url": "https://example.com",
    "crawler_version": "1.0.0"
  },
  "data": {
    "text": {
      "title": "Example Page",
      "content": "Main content text...",
      "paragraphs": ["æ®µè½1", "æ®µè½2"]
    },
    "links": {
      "internal_links": [...],
      "external_links": [...],
      "total_count": 25
    },
    "images": {
      "images": [...],
      "total_count": 8
    }
  }
}
```

### JSONLå½¢å¼

```yaml
settings:
  output_format: "jsonl"
```

å„è¡ŒãŒ1ã¤ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™ã€‚

## å®Ÿç”¨ä¾‹

### 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³åé›†

```bash
python3 crawler.py fetch https://www3.nhk.or.jp/news/ \
  --screenshot --wait 3000 --output nhk_news.json
```

### 2. ECã‚µã‚¤ãƒˆã®å•†å“æƒ…å ±å–å¾—

```yaml
jobs:
  - name: "product_info"
    url: "https://example-shop.com/products/123"
    selectors:
      title: "h1.product-title"
      price: ".price"
      description: ".product-description"
      images: ".product-images img"
    wait: 2000
    extract: ["text", "images"]
```

### 3. ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æŠ•ç¨¿åé›†

```yaml
jobs:
  - name: "social_posts"
    url: "https://example-social.com/hashtag/AI"
    scroll: true
    scroll_count: 5
    wait: 4000
    selectors:
      posts: ".post-content"
      authors: ".post-author"
      timestamps: ".post-time"
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼

#### 1. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼

```yaml
settings:
  timeout: 60000  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ã‚’å»¶é•·
```

#### 2. è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„

```yaml
jobs:
  - name: "example"
    url: "https://example.com"
    wait: 5000      # å¾…æ©Ÿæ™‚é–“ã‚’å»¶é•·
    scroll: true    # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦è¦ç´ ã‚’èª­ã¿è¾¼ã¿
```

#### 3. ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ã‚¨ãƒ©ãƒ¼

```bash
# Playwrightãƒ–ãƒ©ã‚¦ã‚¶ã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
python3 -m playwright install chromium
```

### ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰

```bash
# ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¡¨ç¤ºã—ã¦ãƒ‡ãƒãƒƒã‚°
python3 crawler.py fetch https://example.com --headful
```

```yaml
debug:
  verbose: true
  headful: true
  devtools: true
```

## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
teddy_crawler/
â”œâ”€â”€ README.md              # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ crawler.py             # ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³
â”œâ”€â”€ config.yaml            # ã‚µãƒ³ãƒ—ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ extractors/            # ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text.py            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
â”‚   â”œâ”€â”€ links.py           # ãƒªãƒ³ã‚¯æŠ½å‡º
â”‚   â””â”€â”€ images.py          # ç”»åƒæŠ½å‡º
â”œâ”€â”€ storage/               # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ json_store.py      # JSONä¿å­˜
â”œâ”€â”€ profiles/              # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
â””â”€â”€ output/                # ã‚¯ãƒ­ãƒ¼ãƒ«çµæœå‡ºåŠ›å…ˆ
    â”œâ”€â”€ crawl_20240115_103000_example.json
    â”œâ”€â”€ batch_crawl_20240115_103500.json
    â””â”€â”€ summary_report_20240115_104000.json
```

## è²¢çŒ®ãƒ»é–‹ç™º

### æ©Ÿèƒ½æ‹¡å¼µ

æ–°ã—ã„æŠ½å‡ºæ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹å ´åˆã¯ã€`extractors/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ–°ã—ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

### ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸

JSONä»¥å¤–ã®ä¿å­˜å½¢å¼ãŒå¿…è¦ãªå ´åˆã¯ã€`storage/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ»æ³¨æ„äº‹é …

### åˆ©ç”¨æ™‚ã®æ³¨æ„

1. **robots.txt ã‚’ç¢ºèª**: ã‚µã‚¤ãƒˆã®ã‚¯ãƒ­ãƒ¼ãƒ«å¯å¦ã‚’äº‹å‰ç¢ºèª
2. **ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è¨­å®š**: ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’é¿ã‘ã‚‹
3. **åˆ©ç”¨è¦ç´„ã‚’éµå®ˆ**: å„ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã«å¾“ã†
4. **è‘—ä½œæ¨©ã«æ³¨æ„**: åé›†ãƒ‡ãƒ¼ã‚¿ã®åˆ©ç”¨æ–¹æ³•ã«æ³¨æ„

### æ¨å¥¨è¨­å®š

```yaml
settings:
  delay: 2000    # æœ€ä½2ç§’é–“éš”
  timeout: 30000 # é©åˆ‡ãªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
  user_agent: "Your-Bot-Name/1.0 (contact@example.com)"  # é€£çµ¡å…ˆã‚’å«ã‚€
```

---

Happy Crawling! ğŸ»ğŸ•¸ï¸